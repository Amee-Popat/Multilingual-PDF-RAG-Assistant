import requests
from backend.embeddings import generate_embeddings

def call_ollama(prompt):
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": "mistral",
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.0
            }
        }
    )
    return response.json()["response"]


def generate_answer(collection, question, language):

    # üîç Embed the question
    question_embedding = generate_embeddings([question])[0]

    # üîé Retrieve relevant chunks
    results = collection.query(
        query_embeddings=[question_embedding],
        n_results=3
    )

    docs = results.get("documents", [[]])[0]
    context = "\n\n".join(docs)

    if language == "hindi":
        instruction = """
‡§Ü‡§™ ‡§è‡§ï ‡§∏‡§ñ‡•ç‡§§ ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º ‡§™‡•ç‡§∞‡§∂‡•ç‡§®-‡§â‡§§‡•ç‡§§‡§∞ ‡§∏‡§π‡§æ‡§Ø‡§ï ‡§π‡•à‡§Ç‡•§
‡§ï‡•á‡§µ‡§≤ ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º ‡§Æ‡•á‡§Ç ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§≤‡§ø‡§ñ‡•Ä ‡§ó‡§à ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç‡•§
‡§ñ‡•Å‡§¶ ‡§∏‡•á ‡§ï‡•ã‡§à ‡§ó‡§£‡§®‡§æ ‡§® ‡§ï‡§∞‡•á‡§Ç‡•§
Opening balance ‡§ï‡•ã credit ‡§® ‡§Æ‡§æ‡§®‡•á‡§Ç‡•§
‡§Ø‡§¶‡§ø ‡§ï‡•Å‡§≤ ‡§∞‡§æ‡§∂‡§ø ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§®‡§π‡•Ä‡§Ç ‡§¶‡•Ä ‡§ó‡§à ‡§π‡•à, ‡§§‡•ã ‡§ï‡•á‡§µ‡§≤ ‡§¶‡§ø‡§è ‡§ó‡§è credit amount ‡§ï‡•ã ‡§≤‡§ø‡§ñ‡•á‡§Ç‡•§
‡§â‡§§‡•ç‡§§‡§∞ ‡§õ‡•ã‡§ü‡§æ ‡§î‡§∞ ‡§§‡§•‡•ç‡§Ø‡§æ‡§§‡•ç‡§Æ‡§ï ‡§∞‡§ñ‡•á‡§Ç‡•§
‡§ï‡•á‡§µ‡§≤ ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•á‡§Ç‡•§
"""


    else:
        instruction = """
You are a strict document question-answering assistant.
Answer ONLY using explicitly stated facts from the context.
Do NOT perform calculations unless the result is explicitly written in the document.
Do NOT include opening balance when asked about credits.
If aggregation is required and not explicitly stated, answer with the exact amounts listed.
Keep the answer short and factual.
Respond in English only.
"""


    prompt = f"""
{instruction}

Context:
{context}

Question:
{question}

Answer:
"""

    return call_ollama(prompt).strip()
